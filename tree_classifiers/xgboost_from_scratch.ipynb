{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7832\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Step 1: Implement a Decision Tree (CART) ---\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1, min_samples_split=2, regularization=0.01):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.regularization = regularization\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, gradients, hessians, depth=0):\n",
    "        m, n = X.shape\n",
    "\n",
    "        # Stopping condition\n",
    "        if depth >= self.max_depth or m < self.min_samples_split:\n",
    "            return -np.sum(gradients) / (np.sum(hessians) + self.regularization)  # Newton-Raphson leaf value\n",
    "\n",
    "        # Randomly select a subset of features\n",
    "        num_features = max(2, int(np.sqrt(n)))\n",
    "        selected_features = np.random.choice(n, num_features, replace=False)\n",
    "\n",
    "        # Find best split\n",
    "        best_feature, best_threshold, best_gain = None, None, -float(\"inf\")\n",
    "        for feature in selected_features:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = X[:, feature] > threshold\n",
    "\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = ((np.sum(gradients[left_mask]) ** 2) / (np.sum(hessians[left_mask]) + self.regularization) +\n",
    "                        (np.sum(gradients[right_mask]) ** 2) / (np.sum(hessians[right_mask]) + self.regularization) -\n",
    "                        (np.sum(gradients) ** 2) / (np.sum(hessians) + self.regularization))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # Stop splitting if no gain\n",
    "        if best_gain < self.regularization:\n",
    "            return -np.sum(gradients) / (np.sum(hessians) + self.regularization)\n",
    "\n",
    "        # Create tree structure\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = X[:, best_feature] > best_threshold\n",
    "\n",
    "        return {\n",
    "            'feature_index': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': self.fit(X[left_mask], y[left_mask], gradients[left_mask], hessians[left_mask], depth + 1),\n",
    "            'right': self.fit(X[right_mask], y[right_mask], gradients[right_mask], hessians[right_mask], depth + 1)\n",
    "        }\n",
    "\n",
    "    def predict_sample(self, sample, node):\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "        if sample[node['feature_index']] <= node['threshold']:\n",
    "            return self.predict_sample(sample, node['left'])\n",
    "        else:\n",
    "            return self.predict_sample(sample, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(sample, self.tree) for sample in X])\n",
    "\n",
    "\n",
    "# --- Step 2: Implement XGBoost with Gradient Boosting & Log Loss ---\n",
    "class XGBoostFromScratch:\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.1, max_depth=5, min_samples_split=10, regularization=0.01):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.regularization = regularization\n",
    "        self.trees = []\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        prior_log_odds = np.log(np.mean(y) / (1 - np.mean(y)))\n",
    "        predictions = np.full(m, prior_log_odds)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            probabilities = np.clip(self.sigmoid(predictions), 1e-6, 1 - 1e-6)\n",
    "            gradients = probabilities - y  # First derivative\n",
    "            hessians = probabilities * (1 - probabilities)  # Second derivative\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, regularization=self.regularization)\n",
    "            tree.tree = tree.fit(X, y, gradients, hessians)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "        return (self.sigmoid(pred) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# --- Step 3: Load Titanic Dataset Locally and Train Model ---\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"../data/titanic.csv\"  # Ensure this file exists\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df = df[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n",
    "    df.dropna(inplace=True)\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X = df.drop(columns=[\"Survived\"]).values\n",
    "    y = df[\"Survived\"].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model = XGBoostFromScratch(n_estimators=20, learning_rate=0.05, max_depth=8, min_samples_split=10, regularization=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
